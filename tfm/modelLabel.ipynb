{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3W dataset's General Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a general presentation of the 3W dataset, to the best of its authors' knowledge, the first realistic and public dataset with rare undesirable real events in oil wells that can be readily used as a benchmark dataset for development of machine learning techniques related to inherent difficulties of actual data.\n",
    "\n",
    "For more information about the theory behind this dataset, refer to the paper **A Realistic and Public Dataset with Rare Undesirable Real Events in Oil Wells** published in the **Journal of Petroleum Science and Engineering** (link [here](https://doi.org/10.1016/j.petrol.2019.106223))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook presents the 3W dataset in a general way. For this, some tables, graphs, and statistics are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "sys.path.append(os.path.join('..','..'))\n",
    "import toolkit as tk\n",
    "\n",
    "from itertools import (takewhile,repeat)\n",
    "import bisect\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Instances' Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, all 3W dataset's instances are loaded and the first one of each knowledge source (real, simulated and hand-drawn) is partially displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawincount(filename):\n",
    "    '''https://stackoverflow.com/questions/845058/how-to-get-line-count-of-a-large-file-cheaply-in-python'''\n",
    "    f = open(filename, 'rb')\n",
    "    bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "    return sum( buf.count(b'\\n') for buf in bufgen )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_instances, simulated_instances, drawn_instances = tk.get_all_labels_and_files()\n",
    "\n",
    "real_instances = pd.DataFrame(real_instances, columns=['label', 'path'])\n",
    "real_instances['nlines'] = real_instances['path'].apply(rawincount)\n",
    "\n",
    "simulated_instances = pd.DataFrame(simulated_instances, columns=['label', 'path'])\n",
    "simulated_instances['nlines'] = simulated_instances['path'].apply(rawincount)\n",
    "\n",
    "drawn_instances = pd.DataFrame(drawn_instances, columns=['label', 'path'])\n",
    "drawn_instances['nlines'] = drawn_instances['path'].apply(rawincount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = sklearn.model_selection.train_test_split(real_instances, test_size=0.2, \n",
    "                                                random_state=200560, shuffle=True, \n",
    "                                                stratify=real_instances['label'])\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P-PDG</th>\n",
       "      <th>P-TPT</th>\n",
       "      <th>T-TPT</th>\n",
       "      <th>P-MON-CKP</th>\n",
       "      <th>T-JUS-CKP</th>\n",
       "      <th>P-JUS-CKGL</th>\n",
       "      <th>T-JUS-CKGL</th>\n",
       "      <th>QGL</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-09 05:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16846860.0</td>\n",
       "      <td>118.0778</td>\n",
       "      <td>7802836.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4106967.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 05:00:01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16846970.0</td>\n",
       "      <td>118.0775</td>\n",
       "      <td>7796857.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4107032.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 05:00:02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16847080.0</td>\n",
       "      <td>118.0772</td>\n",
       "      <td>7790877.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4107097.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 05:00:03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16847190.0</td>\n",
       "      <td>118.0769</td>\n",
       "      <td>7784897.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4107162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 05:00:04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16847310.0</td>\n",
       "      <td>118.0765</td>\n",
       "      <td>7778917.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4107228.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 06:59:45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16820820.0</td>\n",
       "      <td>118.0437</td>\n",
       "      <td>7568130.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4524903.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 06:59:46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16820980.0</td>\n",
       "      <td>118.0437</td>\n",
       "      <td>7571650.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4524937.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 06:59:47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16821150.0</td>\n",
       "      <td>118.0437</td>\n",
       "      <td>7575171.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4524969.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 06:59:48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16821320.0</td>\n",
       "      <td>118.0437</td>\n",
       "      <td>7578691.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4525003.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-09 06:59:49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16821480.0</td>\n",
       "      <td>118.0437</td>\n",
       "      <td>7582211.0</td>\n",
       "      <td>173.0961</td>\n",
       "      <td>4525036.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7190 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     P-PDG       P-TPT     T-TPT  P-MON-CKP  T-JUS-CKP  \\\n",
       "timestamp                                                                \n",
       "2013-12-09 05:00:00    0.0  16846860.0  118.0778  7802836.0   173.0961   \n",
       "2013-12-09 05:00:01    0.0  16846970.0  118.0775  7796857.0   173.0961   \n",
       "2013-12-09 05:00:02    0.0  16847080.0  118.0772  7790877.0   173.0961   \n",
       "2013-12-09 05:00:03    0.0  16847190.0  118.0769  7784897.0   173.0961   \n",
       "2013-12-09 05:00:04    0.0  16847310.0  118.0765  7778917.0   173.0961   \n",
       "...                    ...         ...       ...        ...        ...   \n",
       "2013-12-09 06:59:45    0.0  16820820.0  118.0437  7568130.0   173.0961   \n",
       "2013-12-09 06:59:46    0.0  16820980.0  118.0437  7571650.0   173.0961   \n",
       "2013-12-09 06:59:47    0.0  16821150.0  118.0437  7575171.0   173.0961   \n",
       "2013-12-09 06:59:48    0.0  16821320.0  118.0437  7578691.0   173.0961   \n",
       "2013-12-09 06:59:49    0.0  16821480.0  118.0437  7582211.0   173.0961   \n",
       "\n",
       "                     P-JUS-CKGL  T-JUS-CKGL  QGL  class  \n",
       "timestamp                                                \n",
       "2013-12-09 05:00:00   4106967.0         NaN  0.0      4  \n",
       "2013-12-09 05:00:01   4107032.0         NaN  0.0      4  \n",
       "2013-12-09 05:00:02   4107097.0         NaN  0.0      4  \n",
       "2013-12-09 05:00:03   4107162.0         NaN  0.0      4  \n",
       "2013-12-09 05:00:04   4107228.0         NaN  0.0      4  \n",
       "...                         ...         ...  ...    ...  \n",
       "2013-12-09 06:59:45   4524903.0         NaN  0.0      4  \n",
       "2013-12-09 06:59:46   4524937.0         NaN  0.0      4  \n",
       "2013-12-09 06:59:47   4524969.0         NaN  0.0      4  \n",
       "2013-12-09 06:59:48   4525003.0         NaN  0.0      4  \n",
       "2013-12-09 06:59:49   4525036.0         NaN  0.0      4  \n",
       "\n",
       "[7190 rows x 9 columns]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset/4/WELL-00002_20131209050000.csv', index_col=\"timestamp\", parse_dates=[\"timestamp\"])\n",
    "flist0 = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'P-JUS-CKGL', 'T-JUS-CKGL', 'QGL', 'class']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance is stored in a CSV file and loaded into a pandas DataFrame. Each observation is stored in a line in the CSV file and loaded as a line in the pandas DataFrame. The first line of each CSV file contains a header with column identifiers. Each column of CSV files stores the following type of information:\n",
    "\n",
    "* **timestamp**: observations timestamps loaded into pandas DataFrame as its index;\n",
    "* **P-PDG**: pressure variable at the Permanent Downhole Gauge (PDG);\n",
    "* **P-TPT**: pressure variable at the Temperature and Pressure Transducer (TPT);\n",
    "* **T-TPT**: temperature variable at the Temperature and Pressure Transducer (TPT);\n",
    "* **P-MON-CKP**: pressure variable upstream of the production choke (CKP);\n",
    "* **T-JUS-CKP**: temperature variable downstream of the production choke (CKP);\n",
    "* **P-JUS-CKGL**: pressure variable upstream of the gas lift choke (CKGL);\n",
    "* **T-JUS-CKGL**: temperature variable upstream of the gas lift choke (CKGL);\n",
    "* **QGL**: gas lift flow rate;\n",
    "* **class**: observations labels associated with three types of periods (normal, fault transient, and faulty steady state).\n",
    "\n",
    "Other information are also loaded into each pandas Dataframe:\n",
    "\n",
    "* **label**: instance label (event type);\n",
    "* **well**: well name. Hand-drawn and simulated instances have fixed names. Real instances have names masked with incremental id;\n",
    "* **id**: instance identifier. Hand-drawn and simulated instances have incremental id. Each real instance has an id generated from its first timestamp.\n",
    "\n",
    "More information about these variables can be obtained from the following publicly available documents:\n",
    "\n",
    "* ***Option in Portuguese***: R.E.V. Vargas. Base de dados e benchmarks para prognóstico de anomalias em sistemas de elevação de petróleo. Universidade Federal do Espírito Santo. Doctoral thesis. 2019. https://github.com/ricardovvargas/3w_dataset/raw/master/docs/doctoral_thesis_ricardo_vargas.pdf.\n",
    "* ***Option in English***: B.G. Carvalho. Evaluating machine learning techniques for detection of flow instability events in offshore oil wells. Universidade Federal do Espírito Santo. Master's degree dissertation. 2021. https://github.com/ricardovvargas/3w_dataset/raw/master/docs/master_degree_dissertation_bruno_carvalho.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows the amount of instances that compose the 3W dataset, by knowledge source (real, simulated and hand-drawn instances) and by instance label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7190 119\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset/4/WELL-00002_20131209050000.csv', index_col=\"timestamp\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "if np.any(df['class'].isna()):\n",
    "    df['class'] = df['class'].fillna(method='ffill')\n",
    "df['class'] = df['class'].astype('int')\n",
    "\n",
    "\n",
    "flist0 = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'P-JUS-CKGL', 'T-JUS-CKGL', 'QGL', 'class']\n",
    "flist = []\n",
    "for f in flist0:\n",
    "    if np.sum(df[f].isna()) < len(df.index) * 0.2:\n",
    "        flist.append(f)\n",
    "\n",
    "fdict=dict()\n",
    "for f in flist:\n",
    "    fdict[f] = ['mean','std']\n",
    "\n",
    "def mode(series):\n",
    "    return pd.Series.mode(series)[0]\n",
    "    \n",
    "fdict['class'] = [mode]\n",
    "    \n",
    "df['minute'] = (df.index-df.index[0])//np.timedelta64(1,'m')\n",
    "\n",
    "#ds = df.groupby('minute').agg(fdict).dropna().iloc[:-1]\n",
    "\n",
    "ds = df.groupby('minute').agg(fdict)\n",
    "\n",
    "if df.index[-1].second != 59:\n",
    "    ds = ds.iloc[:-1]\n",
    "\n",
    "print(len(df.index), len(ds.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['class'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in flist0:\n",
    "    if f not in flist:\n",
    "        ds[f, 'mean'] = np.NaN\n",
    "        ds[f, 'std'] = np.NaN\n",
    "ds = ds[flist0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ds(ds, flist):\n",
    "    fig, axs = plt.subplots(nrows=len(flist), figsize=(12, 12), sharex=True)\n",
    "\n",
    "    for i, vs in enumerate(flist[:-1]):\n",
    "        axs[i].plot(ds.index, ds[(vs, 'mean')])\n",
    "        axs[i].fill_between(ds.index, ds[(vs, 'mean')]-1.96*ds[(vs, 'std')], \n",
    "                        ds[(vs, 'mean')]+1.96*ds[(vs, 'std')], \n",
    "                        alpha=0.2)\n",
    "        axs[i].set_ylabel(vs)\n",
    "    axs[i+1].plot(ds.index, ds[flist[-1]])\n",
    "    axs[i+1].set_ylabel(flist[-1])\n",
    "    axs[i+1].set_xlabel('minute')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15\n",
    "ts = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    ds.drop('class', axis=1),\n",
    "    ds['class'].iloc[seq_length-1:].append(ds['class'].iloc[:seq_length-1]).reset_index(drop=True),\n",
    "    sequence_length=seq_length,\n",
    "    sequence_stride=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    start_index=None,\n",
    "    end_index=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    np.arange(100).reshape((50, 2)),\n",
    "    -np.arange(14,65),\n",
    "    sequence_length=15,\n",
    "    sequence_stride=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    start_index=None,\n",
    "    end_index=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    '''https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3'''\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col, categories,\n",
    "                 batch_size,\n",
    "                 seq_length=15):\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.categories = categories\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.file = None\n",
    "        self.ts = None\n",
    "        \n",
    "        self.n = self.__calc_n()\n",
    "    \n",
    "    def __calc_n(self):\n",
    "        self.df['nbatches'] = np.int32(np.ceil(((self.df['nlines'] // 60)-self.seq_length)/self.batch_size))\n",
    "        self.df['ibatch'] = self.df['nbatches'].cumsum() - 1\n",
    "        return int(self.df['nbatches'].sum())\n",
    "    \n",
    "    def plot(self, ifile):\n",
    "        \n",
    "        ds = self.__get_ds(self.df['path'][ifile], Norm=False)\n",
    "        \n",
    "        fig, axs = plt.subplots(nrows=len(flist)+1, figsize=(10, 12), sharex=True)\n",
    "        \n",
    "        fig.suptitle(self.df['path'][ifile])\n",
    "\n",
    "        for i, vs in enumerate(self.X_col):\n",
    "            axs[i].plot(ds.index, ds[(vs, 'mean')])\n",
    "            axs[i].fill_between(ds.index, ds[(vs, 'mean')]-1.96*ds[(vs, 'std')], \n",
    "                            ds[(vs, 'mean')]+1.96*ds[(vs, 'std')], \n",
    "                            alpha=0.2)\n",
    "            axs[i].set_ylabel(vs)\n",
    "            axs[i].grid()\n",
    "        \n",
    "        id = np.argsort(ds[(self.y_col, 'mode')])\n",
    "        \n",
    "        axs[i+1].scatter([ds.index[i] for i in id], [str(ds[(self.y_col, 'mode')][i]) for i in id], marker='.')\n",
    "        \n",
    "        axs[i+1].set_ylabel(self.y_col)\n",
    "        \n",
    "        axs[i+1].set_xlabel('minute')\n",
    "\n",
    "        plt.show()    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "            pass\n",
    "    \n",
    "    def __get_ds(self, p, Norm=True):\n",
    "    \n",
    "        dfo = pd.read_csv(p, index_col=\"timestamp\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "        if np.any(dfo[self.y_col].isna()):\n",
    "            dfo[self.y_col] = dfo[self.y_col].fillna(method='ffill')\n",
    "        dfo[self.y_col] = dfo[self.y_col].astype('int')\n",
    "        \n",
    "        flist = []\n",
    "        flist0 = []\n",
    "        for f in self.X_col:\n",
    "            nas = np.sum(dfo[f].isna())\n",
    "            if nas > 0:\n",
    "                if nas < len(dfo.index) * 0.2:\n",
    "                    dfo[f] = dfo[f].fillna(method='ffill')\n",
    "                    flist.append(f)\n",
    "                else:\n",
    "                    flist0.append(f)\n",
    "            else:\n",
    "                flist.append(f)\n",
    "\n",
    "        fdict=dict()\n",
    "        for f in flist:\n",
    "            fdict[f] = ['mean','std']\n",
    "\n",
    "        def mode(series):\n",
    "            return pd.Series.mode(series)[0]\n",
    "\n",
    "        fdict[self.y_col] = [mode]\n",
    "\n",
    "        dfo['minute'] = (dfo.index-dfo.index[0])//np.timedelta64(1,'m')\n",
    "\n",
    "        ds = dfo.groupby('minute').agg(fdict)\n",
    "\n",
    "        if dfo.index[-1].second != 59:\n",
    "            ds = ds.iloc[:-1]\n",
    "\n",
    "        for f in flist0:\n",
    "            ds[f, 'mean'] = np.NaN\n",
    "            ds[f, 'std'] = np.NaN\n",
    "        \n",
    "        if Norm:\n",
    "            ds = self.__Norm(ds)\n",
    "        \n",
    "        return ds[self.X_col + [self.y_col]]\n",
    "        \n",
    "    def __Norm(self, ds, nas_v=0):\n",
    "        dn = ds.fillna(value=nas_v)\n",
    "        sc = sklearn.preprocessing.StandardScaler()\n",
    "        dn = pd.DataFrame(sc.fit_transform(dn), \n",
    "                                           index=dn.index, \n",
    "                                           columns=dn.columns)\n",
    "        dn[(self.y_col, 'mode')] = ds[(self.y_col, 'mode')]\n",
    "        return dn\n",
    "    \n",
    "    def __get_dt(self, p):\n",
    "\n",
    "        ds = self.__get_ds(p)\n",
    "        \n",
    "        self.ts = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            ds.drop(self.y_col, axis=1, level=0),\n",
    "            ds[self.y_col].iloc[self.seq_length-1:].append(ds[self.y_col].iloc[:self.seq_length-1]).reset_index(drop=True),\n",
    "            sequence_length=self.seq_length,\n",
    "            sequence_stride=1,\n",
    "            sampling_rate=1,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            seed=None,\n",
    "            start_index=None,\n",
    "            end_index=None\n",
    "        )        \n",
    "        \n",
    "        self.ts = list(self.ts)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __get_output(self, y):\n",
    "        \n",
    "        ohe = sklearn.preprocessing.OneHotEncoder(categories=self.categories, sparse= False)\n",
    "        \n",
    "        return ohe.fit_transform(y)\n",
    "    \n",
    "    def __get_data(self, i, j, p):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        if p != self.file:\n",
    "            self.__get_dt(p)\n",
    "\n",
    "        return self.ts[j]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        i = bisect.bisect_left(self.df.ibatch, index)\n",
    "        if i > 0:\n",
    "            j = index - self.df.ibatch[i-1] - 1\n",
    "        else:\n",
    "            j = index\n",
    "        \n",
    "        p = self.df.path[i]       \n",
    "        \n",
    "        #print(index, i, j, p)\n",
    "        \n",
    "        X, y = self.__get_data(i, j, p)        \n",
    "        \n",
    "        return X, self.__get_output(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist0 = ['P-PDG', 'P-TPT', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'P-JUS-CKGL', 'T-JUS-CKGL', 'QGL']\n",
    "categories=[[0,1,2,3,4,5,6,7,8,101,102,103,104,105,106,107,108]]\n",
    "train = CustomDataGen(train_df, flist0, 'class', categories, 32, 15)\n",
    "val = CustomDataGen(val_df, flist0, 'class', categories, 32, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>nlines</th>\n",
       "      <th>nbatches</th>\n",
       "      <th>ibatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>..\\..\\dataset\\0\\WELL-00002_20170215180118.csv</td>\n",
       "      <td>17923</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>..\\..\\dataset\\0\\WELL-00002_20170621030054.csv</td>\n",
       "      <td>17947</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>..\\..\\dataset\\4\\WELL-00010_20180425040224.csv</td>\n",
       "      <td>7057</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>..\\..\\dataset\\0\\WELL-00001_20170219120021.csv</td>\n",
       "      <td>17980</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>..\\..\\dataset\\4\\WELL-00002_20131215000010.csv</td>\n",
       "      <td>7191</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>4</td>\n",
       "      <td>..\\..\\dataset\\4\\WELL-00010_20180425120029.csv</td>\n",
       "      <td>7172</td>\n",
       "      <td>4</td>\n",
       "      <td>5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0</td>\n",
       "      <td>..\\..\\dataset\\0\\WELL-00001_20170219170053.csv</td>\n",
       "      <td>17948</td>\n",
       "      <td>9</td>\n",
       "      <td>5686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0</td>\n",
       "      <td>..\\..\\dataset\\0\\WELL-00005_20170401170000.csv</td>\n",
       "      <td>17957</td>\n",
       "      <td>9</td>\n",
       "      <td>5695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>5</td>\n",
       "      <td>..\\..\\dataset\\5\\WELL-00015_20171013140047.csv</td>\n",
       "      <td>7755</td>\n",
       "      <td>4</td>\n",
       "      <td>5699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>4</td>\n",
       "      <td>..\\..\\dataset\\4\\WELL-00001_20170316160005.csv</td>\n",
       "      <td>7170</td>\n",
       "      <td>4</td>\n",
       "      <td>5703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>817 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           path  nlines  nbatches  \\\n",
       "0        0  ..\\..\\dataset\\0\\WELL-00002_20170215180118.csv   17923         9   \n",
       "1        0  ..\\..\\dataset\\0\\WELL-00002_20170621030054.csv   17947         9   \n",
       "2        4  ..\\..\\dataset\\4\\WELL-00010_20180425040224.csv    7057         4   \n",
       "3        0  ..\\..\\dataset\\0\\WELL-00001_20170219120021.csv   17980         9   \n",
       "4        4  ..\\..\\dataset\\4\\WELL-00002_20131215000010.csv    7191         4   \n",
       "..     ...                                            ...     ...       ...   \n",
       "812      4  ..\\..\\dataset\\4\\WELL-00010_20180425120029.csv    7172         4   \n",
       "813      0  ..\\..\\dataset\\0\\WELL-00001_20170219170053.csv   17948         9   \n",
       "814      0  ..\\..\\dataset\\0\\WELL-00005_20170401170000.csv   17957         9   \n",
       "815      5  ..\\..\\dataset\\5\\WELL-00015_20171013140047.csv    7755         4   \n",
       "816      4  ..\\..\\dataset\\4\\WELL-00001_20170316160005.csv    7170         4   \n",
       "\n",
       "     ibatch  \n",
       "0         8  \n",
       "1        17  \n",
       "2        21  \n",
       "3        30  \n",
       "4        34  \n",
       "..      ...  \n",
       "812    5677  \n",
       "813    5686  \n",
       "814    5695  \n",
       "815    5699  \n",
       "816    5703  \n",
       "\n",
       "[817 rows x 5 columns]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.__get_dt(real_instances[real_instances.label==2].iloc[1].path)\n",
    "for index in range(train.__len__()):\n",
    "    train.__getitem__(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, train, val, patience=2, lr=0.001):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "    history = model.fit(train, epochs=MAX_EPOCHS,\n",
    "                        validation_data=val,\n",
    "                        callbacks=[early_stopping])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_10 (Flatten)        (None, 240)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 17)                4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,097\n",
      "Trainable params: 4,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(15, 16)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=17, activation='sigmoid')\n",
    "])\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5704/5704 [==============================] - 1494s 262ms/step - loss: nan - categorical_accuracy: 0.7358 - val_loss: nan - val_categorical_accuracy: 0.6992\n",
      "Epoch 2/20\n",
      "4797/5704 [========================>.....] - ETA: 3:30 - loss: nan - categorical_accuracy: 0.7420"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-417-fa8e110638fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mval_performance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompile_and_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_performance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-415-4e3cb8922816>\u001b[0m in \u001b[0;36mcompile_and_fit\u001b[1;34m(model, train, val, patience, lr)\u001b[0m\n\u001b[0;32m     10\u001b[0m                   metrics=[tf.keras.metrics.CategoricalAccuracy()])\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     history = model.fit(train, epochs=MAX_EPOCHS,\n\u001b[0m\u001b[0;32m     13\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                         callbacks=[early_stopping])\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_performance = []\n",
    "\n",
    "history = compile_and_fit(linear_model, train, val)\n",
    "\n",
    "val_performance['Linear'] = linear_model.evaluate(val)\n",
    "#performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3WT",
   "language": "python",
   "name": "3wt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
